"""
æ ¸å¿ƒè®ºæ–‡æ¨èä¸æ¨æ–‡ç”Ÿæˆé€»è¾‘
"""

import pandas as pd
from typing import List, Dict, Any
import yaml
from .ai_provider import GeminiProvider

class PaperRecommender:
    def __init__(self, db_path: str = None, gemini_config: str = None):
        import os
        self.db_path = db_path
        # è‡ªåŠ¨æŸ¥æ‰¾API Key
        config_paths = []
        if gemini_config:
            config_paths.append(gemini_config)
        # é¡¹ç›®æ ¹ç›®å½•
        config_paths.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../config_gemini.yaml')))
        config_paths.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../../config_gemini.yaml')))
        api_key = None
        for path in config_paths:
            if os.path.exists(path):
                try:
                    with open(path, "r", encoding="utf-8") as f:
                        config = yaml.safe_load(f)
                        api_key = config.get("gemini_api_key")
                        if api_key:
                            break
                except Exception:
                    continue
        if not api_key:
            print("[ERROR] æœªèƒ½åŠ è½½Gemini API Keyï¼Œè¯·ç¡®ä¿config_gemini.yamlåœ¨é¡¹ç›®æ ¹ç›®å½•ä¸”å†…å®¹æ­£ç¡®ï¼")
        self.gemini = GeminiProvider(api_key) if api_key else None

    def import_csv(self, csv_path: str) -> pd.DataFrame:
        df = pd.read_csv(csv_path)
        return df

    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        # è‡ªåŠ¨è¯†åˆ«å¸¸è§çš„æ ‡é¢˜å’ŒDOIå­—æ®µå
        title_candidates = ["Title", "Article Title", "TI", "æ ‡é¢˜"]
        doi_candidates = ["DOI", "DOI Link", "doi"]
        title_col = next((col for col in title_candidates if col in df.columns), None)
        doi_col = next((col for col in doi_candidates if col in df.columns), None)
        subset = []
        if title_col:
            subset.append(title_col)
        if doi_col:
            subset.append(doi_col)
        if subset:
            df = df.drop_duplicates(subset=subset, keep="first")
        else:
            df = df.drop_duplicates(keep="first")
        return df

    def filter_and_sort(self, df: pd.DataFrame, n: int = 20, ascending: bool = False) -> pd.DataFrame:
        # æŒ‰å¹´ä»½å’Œæ—¥æœŸæ’åºï¼Œascending=Trueä¸ºæœ€æ—©ä¼˜å…ˆï¼ŒFalseä¸ºæœ€æ–°ä¼˜å…ˆ
        if "Publication Year" in df.columns:
            df = df.sort_values(by=["Publication Year", "Publication Date"], ascending=[ascending, ascending])
        return df.head(n)

    def generate_markdown(self, papers: List[Dict[str, Any]], progress_callback=None) -> str:
        import logging
        md = ""
        total = len(papers)
        for idx, paper in enumerate(papers, 1):
            abstract = paper.get('Abstract', '') or paper.get('æ‘˜è¦', '')
            authors = paper.get('Authors', '') or paper.get('Author', '') or paper.get('ä½œè€…', '')
            journal = paper.get('Journal', '') or paper.get('Source title', '') or paper.get('æœŸåˆŠ', '')
            year = paper.get('Publication Year', '') or paper.get('Year', '') or paper.get('å‘è¡¨å¹´ä»½', '')
            doi = paper.get('DOI', '') or paper.get('DOI Link', '') or paper.get('doi', '')
            title = paper.get('Title', '') or paper.get('Article Title', '') or paper.get('TI', '') or paper.get('æ ‡é¢˜', '')
            if not title.strip():
                zh_title = '[ç¼ºå°‘è‹±æ–‡æ ‡é¢˜ï¼Œæ— æ³•ç¿»è¯‘]'
                keywords_en = keywords_zh = highlights = reasons = '[ç¼ºå°‘è‹±æ–‡æ ‡é¢˜ï¼Œæ— æ³•ç”Ÿæˆå†…å®¹]'
                md += f"\n---\n\n**ç¬¬ {idx} ç¯‡**\n\n**ã€è®ºæ–‡æ ‡é¢˜ã€‘**\n[ç¼ºå°‘è‹±æ–‡æ ‡é¢˜ï¼Œæ— æ³•ç¿»è¯‘]\n\n**ã€ä½œè€…ã€‘**\n{authors}\n\n**ã€æœŸåˆŠ/æ¥æºã€‘**\n{journal}, {year}\n{doi}\n\n**ã€æ‘˜è¦ã€‘**\n{abstract}\n\n**ã€å…³é”®è¯ã€‘**\n[ç¼ºå°‘è‹±æ–‡æ ‡é¢˜ï¼Œæ— æ³•ç”Ÿæˆå†…å®¹]\n\n**ã€ç ”ç©¶äº®ç‚¹ã€‘âœ¨**\n[ç¼ºå°‘è‹±æ–‡æ ‡é¢˜ï¼Œæ— æ³•ç”Ÿæˆå†…å®¹]\n\n**ã€æ¨èç†ç”±ã€‘ğŸ‘**\n[ç¼ºå°‘è‹±æ–‡æ ‡é¢˜ï¼Œæ— æ³•ç”Ÿæˆå†…å®¹]\n"
                if progress_callback:
                    progress_callback(idx, total)
                continue

            zh_title = zh_abstract = keywords_en = keywords_zh = highlights = reasons = ""
            try:
                if not self.gemini:
                    raise Exception("Gemini API æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Keyå’Œä¾èµ–ã€‚")
                # 1. æ ‡é¢˜ç¿»è¯‘ï¼ˆåªè¾“å‡ºæ ‡å‡†å­¦æœ¯é£æ ¼ç¿»è¯‘ï¼Œä¸è¦ä»»ä½•è§£é‡Šè¯´æ˜ã€æ¨èã€æ€»ç»“ç­‰å†…å®¹ï¼‰
                zh_title = self.gemini.generate_text(f"è¯·å°†ä¸‹åˆ—è‹±æ–‡è®ºæ–‡æ ‡é¢˜ç¿»è¯‘æˆæ ‡å‡†å­¦æœ¯ä¸­æ–‡ï¼Œä»…è¾“å‡ºç¿»è¯‘æœ¬èº«ï¼š\n{title}")
                # 2. æ‘˜è¦ç¿»è¯‘ï¼ˆåªè¾“å‡ºç¿»è¯‘æœ¬èº«ï¼‰
                zh_abstract = self.gemini.generate_text(f"è¯·å°†ä¸‹åˆ—è‹±æ–‡è®ºæ–‡æ‘˜è¦ç¿»è¯‘æˆæ ‡å‡†å­¦æœ¯ä¸­æ–‡ï¼Œä»…è¾“å‡ºç¿»è¯‘æœ¬èº«ï¼š\n{abstract}")
                # 3. å…³é”®è¯æå–ï¼ˆè‹±æ–‡å…³é”®è¯ï¼Œé€—å·åˆ†éš”ï¼Œä»…è¾“å‡ºå…³é”®è¯æœ¬èº«ï¼‰
                keywords_en = self.gemini.generate_text(f"è¯·ä»ä¸‹åˆ—è‹±æ–‡è®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦ä¸­æå–3-5ä¸ªè‹±æ–‡å…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä»…è¾“å‡ºå…³é”®è¯æœ¬èº«ï¼š\næ ‡é¢˜ï¼š{title}\næ‘˜è¦ï¼š{abstract}")
                # 4. å…³é”®è¯ç¿»è¯‘ï¼ˆä»…è¾“å‡ºä¸­æ–‡å…³é”®è¯æœ¬èº«ï¼‰
                keywords_zh = self.gemini.generate_text(f"è¯·å°†ä¸‹åˆ—è‹±æ–‡å…³é”®è¯ç¿»è¯‘æˆæ ‡å‡†å­¦æœ¯ä¸­æ–‡ï¼Œä»…è¾“å‡ºç¿»è¯‘æœ¬èº«ï¼š{keywords_en}")
                # 5. ç ”ç©¶äº®ç‚¹ï¼ˆç›´æ¥è¾“å‡º1-3æ¡ä¸­æ–‡äº®ç‚¹å†…å®¹ï¼Œæ¯æ¡ç‹¬ç«‹ä¸€è¡Œï¼Œä¸è¦ä»»ä½•å‰ç¼€ã€è§£é‡Šã€è¯´æ˜ã€æ€»ç»“ç­‰ï¼‰
                highlights = self.gemini.generate_text(f"è¯·åŸºäºä¸‹åˆ—è®ºæ–‡æ‘˜è¦ï¼Œç›´æ¥è¾“å‡º1-3æ¡æœ€é‡è¦çš„ç ”ç©¶äº®ç‚¹ï¼Œç”¨ä¸­æ–‡åˆ†ç‚¹ï¼Œæ¯æ¡ç‹¬ç«‹ä¸€è¡Œï¼Œä»…è¾“å‡ºäº®ç‚¹å†…å®¹æœ¬èº«ï¼š\n{abstract}")
                # 6. æ¨èç†ç”±ï¼ˆç›´æ¥è¾“å‡º2-4æ¡ä¸­æ–‡æ¨èç†ç”±ï¼Œæ¯æ¡ç‹¬ç«‹ä¸€è¡Œï¼Œä¸è¦ä»»ä½•å‰ç¼€ã€è§£é‡Šã€è¯´æ˜ã€æ€»ç»“ç­‰ï¼‰
                reasons = self.gemini.generate_text(f"è¯·åŸºäºä¸‹åˆ—è®ºæ–‡å†…å®¹ï¼Œç›´æ¥è¾“å‡º2-4æ¡ç®€æ´æœ‰å¸å¼•åŠ›çš„æ¨èç†ç”±ï¼Œç”¨ä¸­æ–‡åˆ†ç‚¹ï¼Œæ¯æ¡ç‹¬ç«‹ä¸€è¡Œï¼Œä»…è¾“å‡ºç†ç”±å†…å®¹æœ¬èº«ï¼š\næ ‡é¢˜ï¼š{title}\næ‘˜è¦ï¼š{abstract}")
            except Exception as e:
                logging.error(f"Geminiè°ƒç”¨å¤±è´¥: {e}")
                zh_title = zh_abstract = keywords_en = keywords_zh = highlights = reasons = f"[Geminiè°ƒç”¨å¤±è´¥: {e}]"
            # åªè¾“å‡ºä¸€ä¸ªDOI Link
            doi_link = paper.get('DOI Link', '') or (f'https://doi.org/{doi}' if doi and not str(doi).startswith('http') else doi)
            # äº®ç‚¹ä¸æ¨èç†ç”±ç›´æ¥è¾“å‡ºï¼Œæ— å‰ç¼€ã€æ— è§£é‡Šã€æ— å¤šç‰ˆæœ¬
            md += f"\n---\n\n**ç¬¬ {idx} ç¯‡**\n\n**ã€è®ºæ–‡æ ‡é¢˜ã€‘**\n{title}\n{zh_title}\n\n**ã€ä½œè€…ã€‘**\n{authors}\n\n**ã€æœŸåˆŠ/æ¥æºã€‘**\n{journal}, {year}\n{doi_link}\n\n**ã€æ‘˜è¦ã€‘**\n{abstract}\n{zh_abstract}\n\n**ã€å…³é”®è¯ã€‘**\n{keywords_en}\n{keywords_zh}\n\n**ã€ç ”ç©¶äº®ç‚¹ã€‘âœ¨**\n"
            # äº®ç‚¹å¤šè¡Œä¸ºåˆ†ç‚¹è¾“å‡ºï¼ˆå¦‚LLMè¿”å›å¤šè¡Œï¼‰
            if highlights:
                for line in highlights.split('\n'):
                    if line.strip():
                        md += f"*   {line.strip()}\n"
            md += "\n**ã€æ¨èç†ç”±ã€‘ğŸ‘**\n"
            if reasons:
                for line in reasons.split('\n'):
                    if line.strip():
                        md += f"*   {line.strip()}\n"

            if progress_callback:
                progress_callback(idx, total)
        return md
